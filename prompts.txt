4.** Add a **category** field for each metric. **You MUST strictly adhere to the following definitions and examples when assigning a category.** If you identify any other metrics that align with the definitions and are similar in nature to the provided examples for a given category, you MUST include them under that respective category.

    - **Model Quality:** This category evaluates the **technical performance and output accuracy** of the application's core functionality. This includes correctness, completeness, relevance, factual accuracy, instruction adherence, and detection of technical errors like hallucinations. **Crucial examples for this category are:** summary completeness, summarization quality, non-informativeness, summary contradiction check, factual accuracy, relevance scoring, instruction following accuracy, response coherence, and any other metric that directly measures how well the application performs its intended technical task.

    - **Trustworthy:** This category evaluates the **safety, ethics, and responsible AI behavior** of the application. This covers content safety, bias detection, privacy protection, harmful content identification, and compliance with ethical guidelines. **Crucial examples for this category are:** toxicity score, PII detection, profanity rate, fairness metrics, bias assessment, harmful content detection, safety violations, privacy compliance, and any metric that measures potential harm or ethical concerns in the application's behavior.
